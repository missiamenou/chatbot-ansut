# from langchain.text_splitter import RecursiveCharacterTextSplitter
# from langchain_community.vectorstores import FAISS
# from langchain_google_genai import GoogleGenerativeAIEmbeddings

# # √âtape 1 : charger le texte
# with open("base_ansut.txt", "r", encoding="utf-8") as f:
#     full_text = f.read()

# # √âtape 2 : d√©couper le texte en morceaux
# splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
# docs = splitter.create_documents([full_text])

# # √âtape 3 : g√©n√©rer les embeddings
# embeddings = GoogleGenerativeAIEmbeddings(
#     model="models/embedding-001",
#     google_api_key="AIzaSyCchajXnFcyG0zSFo-DPudVdIqgSO0vwrs"  # ‚Üê remplace ici par ta vraie cl√© Google Generative AI
# )

# # √âtape 4 : construire et sauvegarder la base FAISS
# vectordb = FAISS.from_documents(docs, embeddings)
# vectordb.save_local("vectordb_ansut")

# print("‚úÖ Base vectorielle cr√©√©e avec succ√®s.")

# import os
# from langchain.text_splitter import RecursiveCharacterTextSplitter
# from langchain_community.vectorstores import FAISS
# from langchain_google_genai import GoogleGenerativeAIEmbeddings


# def create_vector_database():
#     try:
#         # √âtape 1 : V√©rifier la cl√© API
#         api_key = os.getenv("GOOGLE_API_KEY")
#         if not api_key:
#             raise ValueError("‚ùå Cl√© API Google non trouv√©e. D√©finissez GOOGLE_API_KEY dans vos variables d'environnement.")
        
#         # √âtape 2 : Charger le texte
#         try:
#             with open("base_ansut.txt", "r", encoding="utf-8") as f:
#                 full_text = f.read()
#         except FileNotFoundError:
#             raise FileNotFoundError("‚ùå Fichier 'base_ansut.txt' introuvable")
        
#         # V√©rifier que le fichier n'est pas vide
#         if not full_text.strip():
#             raise ValueError("‚ùå Le fichier est vide")
        
#         print(f"üìÑ Texte charg√© : {len(full_text)} caract√®res")
        
#         # √âtape 3 : D√©couper le texte en morceaux
#         splitter = RecursiveCharacterTextSplitter(
#             chunk_size=500, 
#             chunk_overlap=100,
#             separators=["\n\n", "\n", " ", ""]
#         )
#         docs = splitter.create_documents([full_text])
#         print(f"‚úÇÔ∏è Texte d√©coup√© en {len(docs)} morceaux")
        
#         # √âtape 4 : G√©n√©rer les embeddings
#         embeddings = GoogleGenerativeAIEmbeddings(
#             model="models/embedding-001",
#             google_api_key=api_key
#         )
        
#         # √âtape 5 : Construire et sauvegarder la base FAISS
#         print("üîÑ Cr√©ation de la base vectorielle...")
#         vectordb = FAISS.from_documents(docs, embeddings)
#         vectordb.save_local("vectordb_ansut")
        
#         print("‚úÖ Base vectorielle cr√©√©e avec succ√®s dans 'vectordb_ansut'")
#         return vectordb
        
#     except Exception as e:
#         print(f"‚ùå Erreur lors de la cr√©ation : {e}")
#         return None

# if __name__ == "__main__":
#     create_vector_database()
import os
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
# from langchain.schema import HumanMessage
from langchain.schema import HumanMessage, Document




# def main():
#     # Configuration
#     api_key = os.getenv("GOOGLE_API_KEY")
#     if not api_key:
#         print("‚ùå D√©finissez GOOGLE_API_KEY dans vos variables d'environnement")
#         return
    
#     # 1. Initialiser le chat model
#     chat_model = ChatGoogleGenerativeAI(
#         model="gemini-pro",
#         google_api_key=api_key,
#         temperature=0.7
#     )
    
#     # 2. Initialiser les embeddings
#     embeddings = GoogleGenerativeAIEmbeddings(
#         model="models/embedding-001",
#         google_api_key=api_key
#     )
    
#     # 3. Charger et traiter le texte
#     try:
#         with open("base_ansut.txt", "r", encoding="utf-8") as f:
#             full_text = f.read()
#     except FileNotFoundError:
#         print("‚ùå Fichier 'base_ansut.txt' introuvable")
#         return
    
#     # 4. D√©couper le texte
#     splitter = RecursiveCharacterTextSplitter(
#         chunk_size=500, 
#         chunk_overlap=100
#     )
#     docs = splitter.create_documents([full_text])
    
#     # 5. Cr√©er la base vectorielle
#     try:
#         vectordb = FAISS.from_documents(docs, embeddings)
#         vectordb.save_local("vectordb_ansut")
#         print("‚úÖ Base vectorielle cr√©√©e avec succ√®s")
#     except Exception as e:
#         print(f"‚ùå Erreur lors de la cr√©ation de la base vectorielle : {e}")
#         return
    
#     # 6. Test du chat model
#     try:
#         message = HumanMessage(content="Bonjour, comment allez-vous ?")
#         response = chat_model([message])
#         print(f"ü§ñ R√©ponse du mod√®le : {response.content}")
#     except Exception as e:
#         print(f"‚ùå Erreur lors du test du chat : {e}")

# if __name__ == "__main__":
#     main()

# def main():
#     # Configuration
#     api_key = os.getenv("GOOGLE_API_KEY")
#     if not api_key:
#         print("‚ùå D√©finissez GOOGLE_API_KEY dans vos variables d'environnement")
#         return
    
#     # 1. Initialiser le chat model
#     chat_model = ChatGoogleGenerativeAI(
#         model="gemini-pro",
#         google_api_key=api_key,
#         temperature=0.7
#     )
    
#     # 2. Initialiser les embeddings
#     embeddings = GoogleGenerativeAIEmbeddings(
#         model="models/embedding-001",
#         google_api_key=api_key
#     )
    
#     # 3. Charger et parser base_ansut.txt
#     try:
#         with open("base_ansut.txt", "r", encoding="utf-8") as f:
#             raw_blocks = f.read().split("SOURCE:")
#             docs = []
#             for block in raw_blocks:
#                 block = block.strip()
#                 if not block:
#                     continue
#                 lines = block.split("\n", 1)
#                 url = lines[0].strip()
#                 content = lines[1].strip() if len(lines) > 1 else ""
#                 if content:
#                     docs.append(Document(page_content=content, metadata={"source": url}))
#     except FileNotFoundError:
#         print("‚ùå Fichier 'base_ansut.txt' introuvable")
#         return

#     # 4. D√©couper les documents si n√©cessaire
#     splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
#     split_docs = splitter.split_documents(docs)
    
#     # 5. Cr√©er la base vectorielle avec m√©tadonn√©es
#     try:
#         vectordb = FAISS.from_documents(split_docs, embeddings)
#         vectordb.save_local("vectordb_ansut")
#         print("‚úÖ Base vectorielle cr√©√©e avec succ√®s avec metadata (sources)")
#     except Exception as e:
#         print(f"‚ùå Erreur lors de la cr√©ation de la base vectorielle : {e}")
#         return
    
#     # 6. Test du chat model seul
#     try:
#         message = HumanMessage(content="Bonjour, comment allez-vous ?")
#         response = chat_model([message])
#         print(f"ü§ñ R√©ponse du mod√®le : {response.content}")
#     except Exception as e:
#         print(f"‚ùå Erreur lors du test du chat : {e}")

# if __name__ == "__main__":
#     main()


import os
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.schema import Document
from dotenv import load_dotenv

load_dotenv()

def clean_and_parse_file(file_path: str):
    print("üìÑ Lecture du fichier :", file_path)
    with open(file_path, "r", encoding="utf-8") as f:
        content = f.read()

    blocks = content.split("=== PAGE:")
    documents = []

    for block in blocks:
        block = block.strip()
        if not block:
            continue

        # extraire source et texte
        lines = block.split("\n", 1)
        if len(lines) < 2:
            continue

        url = lines[0].strip()
        texte = lines[1].strip()

        # Nettoyage suppl√©mentaire (optionnel)
        texte = texte.replace("\n", " ").strip()
        if len(texte) < 100:
            continue  # ignorer trop court

        documents.append(Document(page_content=texte, metadata={"source": url}))

    print(f"‚úÖ {len(documents)} documents extraits du site.")
    return documents

def main():
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        print("‚ùå Variable GOOGLE_API_KEY manquante")
        return

    # 1. Embeddings
    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/embedding-001",
        google_api_key=api_key
    )

    # 2. Lecture du fichier scrap√©
    docs = clean_and_parse_file("base_ansut.txt")

    # 3. Split des documents longs
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=100
    )
    split_docs = splitter.split_documents(docs)
    print(f"üß© {len(split_docs)} blocs g√©n√©r√©s apr√®s d√©coupage")

    # 4. Cr√©ation de la base vectorielle
    vectordb = FAISS.from_documents(split_docs, embeddings)
    vectordb.save_local("vectordb_ansut")
    print("‚úÖ Base vectorielle enregistr√©e dans le dossier 'vectordb_ansut'")

if __name__ == "__main__":
    main()
