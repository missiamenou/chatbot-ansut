from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from langchain_community.vectorstores import FAISS
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
import os
from dotenv import load_dotenv
load_dotenv()

# Chargement des variables d'environnement
load_dotenv()

class Question(BaseModel):
    question: str

app = FastAPI()

# CORS pour acc√®s frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# Initialisation des mod√®les
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
if not GOOGLE_API_KEY:
    raise ValueError("‚ùå GOOGLE_API_KEY non d√©fini dans l'environnement")

# Embeddings & vector DB
embeddings = GoogleGenerativeAIEmbeddings(
    model="models/embedding-001",
    google_api_key=GOOGLE_API_KEY
)

db = FAISS.load_local("vectordb_ansut", embeddings, allow_dangerous_deserialization=True)
print("üì¶ Nombre de documents vectoris√©s :", db.index.ntotal)
query = "Qui est le directeur g√©n√©ral de l'ANSUT ?"
print(f"\nüîé TEST: {query}")

results = db.similarity_search(query, k=3)
for i, doc in enumerate(results, 1):
    print(f"\n--- R√©sultat {i} ---")
    print(doc.page_content[:500])
    print("üìé Source :", doc.metadata.get("source"))



# Mod√®le LLM
llm = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash",
    google_api_key=GOOGLE_API_KEY
)

@app.post("/chat")
def chat(req: Question):
    question = req.question

    try:
        docs_and_scores = db.similarity_search_with_score(question, k=3)

        context = ""
        sources = set()

        for doc, _ in docs_and_scores:
            context += doc.page_content + "\n\n"
            source = doc.metadata.get("source")
            if source:
                sources.add(source)

        # Limiter le contexte √† 8000 caract√®res (~5000 tokens)
        MAX_CHARS = 8000
        if len(context) > MAX_CHARS:
            context = context[:MAX_CHARS] + "\n\n[...contenu tronqu√©]"

        prompt = f"""
Tu es un assistant virtuel professionnel de l‚ÄôANSUT, charg√© d‚Äôaccueillir, renseigner et orienter les visiteurs comme une secr√©taire exp√©riment√©e.

Tu t‚Äôexprimes avec courtoisie et clart√©, mais sans saluer inutilement √† chaque r√©ponse. Garde un ton respectueux et professionnel, mais ne r√©p√®te pas "Bonjour" ou "Bienvenue" √† chaque message.

Tu dois r√©pondre uniquement √† partir des informations suivantes, issues du site officiel de l‚ÄôANSUT (https://ansut.ci).  
Si une information n‚Äôest pas disponible dans ce contexte, indique simplement et poliment :  
"Je suis d√©sol√©(e), je n‚Äôai pas trouv√© cette information sur le site de l‚ÄôANSUT."

Tu dois √©galement garder le fil de la conversation, comme le ferait une assistante efficace.

Voici les informations disponibles :  
{context}

Question : {question}
"""


        response = llm.invoke(prompt)

        return {
            "reponse": response.content.strip(),
            "sources": list(sources)
        }

    except Exception as e:
        return {
            "reponse": f"‚ùå Erreur pendant le traitement : {str(e)}",
            "sources": []
        }
