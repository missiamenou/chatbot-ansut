# from fastapi import FastAPI
# from fastapi.middleware.cors import CORSMiddleware
# from pydantic import BaseModel
# from langchain.vectorstores import FAISS
# from langchain_google_genai import GoogleGenerativeAIEmbeddings
# from langchain_google_genai import ChatGoogleGenerativeAI

# class Question(BaseModel):
#     question: str

# app = FastAPI()
# app.add_middleware(
#     CORSMiddleware,
#     allow_origins=["*"],  # üîì autorise toutes les origines (en dev)
#     allow_credentials=True,
#     allow_methods=["*"],
#     allow_headers=["*"],
# )


# # Charger les embeddings + base vectorielle + mod√®le LLM
# embeddings = GoogleGenerativeAIEmbeddings(
#     model="models/embedding-001",
#     google_api_key="AIzaSyCchajXnFcyG0zSFo-DPudVdIqgSO0vwrs"  # ‚Üê remplace par ta vraie cl√©
# )

# db = FAISS.load_local("vectordb_ansut", embeddings, allow_dangerous_deserialization=True)

# llm = ChatGoogleGenerativeAI(
#     model="gemini-1.5-flash",
#     google_api_key="AIzaSyCchajXnFcyG0zSFo-DPudVdIqgSO0vwrs"  # ‚Üê la m√™me cl√©
# )

# @app.post("/chat")
# def chat(req: Question):
#     question = req.question

#     # √âtape 1 : recherche des documents les plus proches
#     docs_and_scores = db.similarity_search_with_score(question, k=3)

#     # √âtape 2 : assembler le contexte
#     context = "\n\n".join([doc.page_content for doc, _ in docs_and_scores])

#     # √âtape 3 : construire le prompt
#     prompt = f"""
# Tu es un assistant virtuel professionnel de l'ANSUT.
# R√©ponds de fa√ßon claire et factuelle en t'appuyant sur les informations suivantes :

# {context}

# Question : {question}
# """

#     # √âtape 4 : g√©n√©ration de la r√©ponse
#     response = llm.invoke(prompt)
#     return {"reponse": response.content}


from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from langchain_community.vectorstores import FAISS  # üëà mise √† jour recommand√©e
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI

class Question(BaseModel):
    question: str

app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialisation
embeddings = GoogleGenerativeAIEmbeddings(
    model="models/embedding-001",
    google_api_key="AIzaSyCchajXnFcyG0zSFo-DPudVdIqgSO0vwrs"
)

db = FAISS.load_local("vectordb_ansut", embeddings, allow_dangerous_deserialization=True)

llm = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash",
    google_api_key="AIzaSyCchajXnFcyG0zSFo-DPudVdIqgSO0vwrs"
)

@app.post("/chat")
def chat(req: Question):
    question = req.question

    # üîç Recherche les 3 documents les plus proches
    docs_and_scores = db.similarity_search_with_score(question, k=3)

    # üîó R√©cup√®re les contenus et leurs sources
    context = ""
    sources = set()

    for doc, _ in docs_and_scores:
        context += doc.page_content + "\n\n"
        source = doc.metadata.get("source")
        if source:
            sources.add(source)

    # üß† Construire le prompt
    prompt = f"""
Tu es un assistant virtuel professionnel de l'ANSUT.
R√©ponds uniquement √† partir des informations suivantes :

{context}

Question : {question}
"""

    # ü§ñ G√©n√©ration de la r√©ponse
    response = llm.invoke(prompt)

    return {
        "reponse": response.content.strip(),
        "sources": list(sources)
    }
